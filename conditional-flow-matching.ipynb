{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4e68e6-508a-4ac7-9579-36c5f0808fa3",
   "metadata": {},
   "source": [
    "\n",
    "BACKEND part should be run or the geomstats does not convert correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7443040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GEOMSTATS_BACKEND=pytorch\n"
     ]
    }
   ],
   "source": [
    "%env GEOMSTATS_BACKEND=pytorch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mesh_to_sdf import sample_sdf_near_surface\n",
    "\n",
    "import trimesh\n",
    "import pyrender\n",
    "import numpy as np\n",
    "import glob \n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252df9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using pytorch backend\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from scipy.spatial.transform import Rotation\n",
    "from geomstats.geometry.special_orthogonal import SpecialOrthogonal\n",
    "\n",
    "# from utils.plotting import plot_so3\n",
    "# from utils.optimal_transport import so3_wasserstein as wasserstein\n",
    "# from FoldFlow.foldflow.utils.so3_helpers import norm_SO3, expmap\n",
    "# from FoldFlow.foldflow.utils.so3_condflowmatcher import SO3ConditionalFlowMatcher\n",
    "# from FoldFlow.so3_experiments.models.models import PMLP\n",
    "\n",
    "from so3_helpers import norm_SO3, expmap\n",
    "from so3_condflowmatcher import SO3ConditionalFlowMatcher\n",
    "from models import PMLP\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "# from data.datasets import SpecialOrthogonalGroup\n",
    "\n",
    "from geomstats._backend import _backend_config as _config\n",
    "_config.DEFAULT_DTYPE = torch.cuda.FloatTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83d3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "so3_group = SpecialOrthogonal(n=3, point_type=\"matrix\")\n",
    "FM = SO3ConditionalFlowMatcher(manifold=so3_group)\n",
    "def loss_fn(v, u, x):\n",
    "    res = v - u\n",
    "    norm = norm_SO3(x, res) # norm-squared on SO(3)\n",
    "    loss = torch.mean(norm, dim=-1)\n",
    "    return loss\n",
    "\n",
    "dim = 9 # network ouput is 9 dimensional (3x3 matrix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# MLP with a projection at the end, projection on to the tanget space of the manifold\n",
    "model = PMLP(dim=dim, time_varying=True).to(device)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# ODE inference on SO(3)\n",
    "def inference(model, xt, t, dt):\n",
    "    with torch.no_grad():\n",
    "        vt = model(torch.cat([xt, t[:, None]], dim=-1)) # vt on the tanget of xt\n",
    "        vt = rearrange(vt, 'b (c d) -> b c d', c=3, d=3)\n",
    "        xt = rearrange(xt, 'b (c d) -> b c d', c=3, d=3)\n",
    "        xt_new = expmap(xt, vt * dt)                   # expmap to get the next point\n",
    "    return rearrange(xt_new, 'b c d -> b (c d)', c=3, d=3)\n",
    "# def inference_recursive(model, x_0, steps=100, device='cuda'):\n",
    "#     t = torch.linspace(0, 1, steps).to(device)\n",
    "#     def ode_func(t, xt,dt):\n",
    "#         # Reshape t to match model input expectations\n",
    "#         t_batch = torch.full((x.shape[0], 1), t.item(), device=device)\n",
    "#         with torch.no_grad():\n",
    "#             vt = model(torch.cat([xt, t[:, None]], dim=-1)) # vt on the tanget of xt\n",
    "#             vt = rearrange(vt, 'b (c d) -> b c d', c=3, d=3)\n",
    "#             xt = rearrange(xt, 'b (c d) -> b c d', c=3, d=3)\n",
    "#             xt_new = expmap(xt, vt * dt)                   # expmap to get the next point\n",
    "#         return rearrange(xt_new, 'b c d -> b (c d)', c=3, d=3)\n",
    "#         #return flow_model(x, t_batch)\n",
    "#     # Integrate from t=0 to t=1\n",
    "#     trajectory = odeint(\n",
    "#         ode_func,\n",
    "#         x_0,\n",
    "#         t,\n",
    "#         method='rk4'  # You can also try 'dopri5' for adaptive stepping\n",
    "#     )\n",
    "    \n",
    "#     return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811f038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example obj:  data/meshes/Radio/ab673912c47afe0afb8c9814b98c66d7.obj\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meshes = glob.glob(\"data/meshes/**/*.obj\")\n",
    "grasps = glob.glob(\"data/grasps/*.h5\")\n",
    "example_obj= meshes[0]\n",
    "example_grasp = grasps[0]\n",
    "\n",
    "\n",
    "example_obj_id = example_obj.split(\"/\")[-1].split(\".\")[0]\n",
    "print(\"Example obj: \", example_obj)\n",
    "\n",
    "corresponding_grasps = [grasp for grasp in grasps if example_obj_id in grasp][0]\n",
    "\n",
    "with h5py.File(example_grasp, 'r') as h5file:\n",
    "    grasp_T = h5file['grasps']['transforms'][0,:,:]\n",
    "grasp_T = torch.tensor(grasp_T).unsqueeze(0).float()\n",
    "grasp_T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d6e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraspDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        so3_part = self.data[idx][:3,:3]\n",
    "        translational_part = self.data[idx][:3,3]\n",
    "        return so3_part, translational_part\n",
    "    \n",
    "grasp_dataset = GraspDataset(grasp_T.double())\n",
    "trainloader = DataLoader(grasp_dataset, batch_size=100, shuffle=True)\n",
    "testset = DataLoader(grasp_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30ae9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [01:14<00:00, 13.47it/s, Epoch=999, Loss=0.0799, Avg Loss=0.0799]\n"
     ]
    }
   ],
   "source": [
    "def main_loop(model, optimizer, num_epochs=150, display=True):\n",
    "    losses = []\n",
    "    global_step = 0\n",
    "    \n",
    "    # Create a single progress bar for all epochs\n",
    "    with tqdm(total=num_epochs * len(trainloader), desc=\"Training\") as global_progress_bar:\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            if (epoch % 10) == 0:\n",
    "                n_test = len(testset.dataset)\n",
    "                traj = torch.tensor(Rotation.random(n_test).as_matrix()).to(device).reshape(-1, 9)\n",
    "                for t in torch.linspace(0, 1, 200):\n",
    "                    t = torch.tensor([t]).to(device).repeat(n_test).requires_grad_(True)\n",
    "                    dt = torch.tensor([1/200]).to(device)\n",
    "                    traj = inference(model, traj, t, dt)\n",
    "                final_traj = rearrange(traj, 'b (c d) -> b c d', c=3, d=3)\n",
    "            \n",
    "            for _, (so3_data, trnslt_part) in enumerate(trainloader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Repeat the data if needed\n",
    "                so3_data = so3_data.repeat(1000, 1, 1)\n",
    "                x1 = so3_data.to(device).double()\n",
    "                x0 = torch.tensor(Rotation.random(x1.size(0)).as_matrix(), dtype=torch.float64).to(device)\n",
    "                \n",
    "                t, xt, ut = FM.sample_location_and_conditional_flow_simple(x0, x1)\n",
    "                \n",
    "                vt = model(torch.cat([rearrange(xt, 'b c d -> b (c d)', c=3, d=3), t[:, None]], dim=-1))\n",
    "                vt = rearrange(vt, 'b (c d) -> b c d', c=3, d=3)\n",
    "                \n",
    "                loss = loss_fn(vt, ut, xt)\n",
    "                epoch_losses.append(loss.detach().item())\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update the global progress bar\n",
    "                global_progress_bar.update(1)\n",
    "                global_progress_bar.set_postfix({\n",
    "                    'Epoch': epoch, \n",
    "                    'Loss': f'{loss.item():.4f}', \n",
    "                    'Avg Loss': f'{np.mean(epoch_losses):.4f}'\n",
    "                })\n",
    "                \n",
    "                global_step += 1\n",
    "    \n",
    "    return model, np.array(losses)\n",
    "\n",
    "# Run training\n",
    "model, losses = main_loop(model, optimizer, num_epochs=1000, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ffb578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5699, -0.0773,  0.8181],\n",
       "          [-0.0769, -0.9862, -0.1467],\n",
       "          [ 0.8181, -0.1465,  0.5561]]], dtype=torch.float64),\n",
       " tensor([[[-0.5708, -0.0766,  0.8175, -0.0758],\n",
       "          [-0.0766, -0.9863, -0.1459,  0.0381],\n",
       "          [ 0.8175, -0.1459,  0.5572, -0.0127]]], dtype=torch.float64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = len(grasp_dataset)\n",
    "traj = torch.tensor(Rotation.random(n_test).as_matrix()).to(device).reshape(-1, 9)\n",
    "for t in torch.linspace(0, 1, 200):\n",
    "    t = torch.tensor([t]).to(device).repeat(n_test)\n",
    "    dt = torch.tensor([1/200]).to(device)\n",
    "    traj = inference(model, traj, t, dt)\n",
    "final_traj = rearrange(traj, 'b (c d) -> b c d', c=3, d=3)\n",
    "final_traj,grasp_dataset.data[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa0c4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE3VelocityField(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64): #trial for translation \n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, hidden_dim),  # Include time t as dim+1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)  # 3 for translation, we will implement SO3 later rt = expr0(tlogr0(r1)) with linalg inverse \n",
    "        )\n",
    "\n",
    "    def forward(self, T, t):\n",
    "        #T_flat = T.view(T.shape[0], -1)  # Flatten T\n",
    "        input_data = torch.cat([T,t ], dim=1)\n",
    "        return self.net(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_flow_matching_loss(flow_model, x):\n",
    "    #Question: Should we calculate one for each time step or generate one time at a time?\n",
    "    \n",
    "    sigma_min = 1e-4\n",
    "    t = torch.rand(x.shape[0], device=x.device).unsqueeze(-1)\n",
    "    noise = torch.randn_like(x).to(x.device)\n",
    "\n",
    "    x_t = (1 - (1 - sigma_min) * t) * noise + t* x\n",
    "    optimal_flow = x - (1 - sigma_min) * noise\n",
    "    predicted_flow = flow_model(x_t, t)\n",
    "\n",
    "    return (predicted_flow - optimal_flow).square().mean()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SE3VelocityField().to(device)\n",
    "x = grasp_T[:, :3, 3]\n",
    "x_train = x.repeat(1000, 1).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100000):\n",
    "    model.zero_grad()\n",
    "    loss = conditional_flow_matching_loss(model,x_train)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}',loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8406c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0713,  0.0321, -0.0082]], grad_fn=<SelectBackward0>) tensor([[-0.0758,  0.0381, -0.0127]])\n"
     ]
    }
   ],
   "source": [
    "def run_flow(flow_model, x_0, steps=100, device='cuda'):\n",
    "    t = torch.linspace(0, 1, steps).to(device)\n",
    "    def ode_func(t, x):\n",
    "        # Reshape t to match model input expectations\n",
    "        t_batch = torch.full((x.shape[0], 1), t.item(), device=device)\n",
    "        return flow_model(x, t_batch)\n",
    "    # Integrate from t=0 to t=1\n",
    "    trajectory = odeint(\n",
    "        ode_func,\n",
    "        x_0,\n",
    "        t,\n",
    "        method='rk4'  # You can also try 'dopri5' for adaptive stepping\n",
    "    )\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "noise = torch.randn_like(grasp_T[:,:3,3]).to(device)\n",
    "trajectory = run_flow(model, noise, steps=100, device=device)\n",
    "print(trajectory[-1],grasp_T[:,:3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level groups: ['grasps', 'gripper', 'object']\n",
      "\n",
      "Exploring complete structure:\n",
      "\n",
      "=== grasps ===\n",
      "Group: qualities\n",
      "  Contents: ['flex']\n",
      "  Group: flex\n",
      "    Contents: ['object_in_gripper', 'object_motion_during_closing_angular', 'object_motion_during_closing_linear', 'object_motion_during_shaking_angular', 'object_motion_during_shaking_linear']\n",
      "    Dataset: object_in_gripper\n",
      "      Shape: (2000,)\n",
      "      Type: int64\n",
      "      First few values: [0 0]\n",
      "    Dataset: object_motion_during_closing_angular\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [0.82173979 0.52632248]\n",
      "    Dataset: object_motion_during_closing_linear\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [0.04771299 0.03616888]\n",
      "    Dataset: object_motion_during_shaking_angular\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [1.77726531 0.487896  ]\n",
      "    Dataset: object_motion_during_shaking_linear\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [0.25422344 0.1152681 ]\n",
      "Dataset: transforms\n",
      "  Shape: (2000, 4, 4)\n",
      "  Type: float64\n",
      "  First few values: [[[-0.57084467 -0.07659983  0.81747711 -0.0758176 ]\n",
      "  [-0.07659983 -0.98632772 -0.14591129  0.0380945 ]\n",
      "  [ 0.81747711 -0.14591129  0.55717238 -0.01268045]\n",
      "  [ 0.          0.          0.          1.        ]]\n",
      "\n",
      " [[-0.57084467  0.34240116  0.74625586 -0.06869548]\n",
      "  [-0.07659983 -0.9271405   0.36680098 -0.01317673]\n",
      "  [ 0.81747711  0.15222331  0.55548108 -0.01251132]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "\n",
      "=== gripper ===\n",
      "Dataset: configuration\n",
      "  Shape: (1,)\n",
      "  Type: float64\n",
      "  First few values: [0.04]\n",
      "Dataset: type\n",
      "  Shape: ()\n",
      "  Type: object\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "\n",
      "=== object ===\n",
      "Dataset: com\n",
      "  Shape: (3,)\n",
      "  Type: float64\n",
      "  First few values: [0.09667312 0.03061758]\n",
      "Dataset: density\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: file\n",
      "  Shape: ()\n",
      "  Type: object\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: friction\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: inertia\n",
      "  Shape: (3, 3)\n",
      "  Type: float64\n",
      "  First few values: [[ 2.18562e-05  5.10000e-09  3.15000e-08]\n",
      " [ 5.10000e-09  9.53581e-05 -2.29900e-07]]\n",
      "Dataset: mass\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: scale\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: volume\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def explore_group(group, indent=\"\"):\n",
    "    \"\"\"Recursively explore an HDF5 group and its contents\"\"\"\n",
    "    for name, item in group.items():\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print(f\"{indent}Group: {name}\")\n",
    "            print(f\"{indent}  Contents: {list(item.keys())}\")\n",
    "            explore_group(item, indent + \"  \")\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(f\"{indent}Dataset: {name}\")\n",
    "            print(f\"{indent}  Shape: {item.shape}\")\n",
    "            print(f\"{indent}  Type: {item.dtype}\")\n",
    "            try:\n",
    "                print(f\"{indent}  First few values: {item[:2]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{indent}  Could not print values: {e}\")\n",
    "\n",
    "with h5py.File(example_grasp, 'r') as h5file:\n",
    "    print(\"Top-level groups:\", list(h5file.keys()))\n",
    "    \n",
    "    print(\"\\nExploring complete structure:\")\n",
    "    for top_group_name in h5file.keys():\n",
    "        print(f\"\\n=== {top_group_name} ===\")\n",
    "        top_group = h5file[top_group_name]\n",
    "        explore_group(top_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
