{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4e68e6-508a-4ac7-9579-36c5f0808fa3",
   "metadata": {},
   "source": [
    "\n",
    "BACKEND part should be run or the geomstats does not convert correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7443040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GEOMSTATS_BACKEND=pytorch\n"
     ]
    }
   ],
   "source": [
    "%env GEOMSTATS_BACKEND=pytorch\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import trimesh\n",
    "import pyrender\n",
    "import numpy as np\n",
    "import glob \n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80a0171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Processing mesh 1/1: 56f2cfa7d89ef32a5eef6c5d029c7274.obj\n",
      "INFO: Mesh ../data/meshes/Plant/56f2cfa7d89ef32a5eef6c5d029c7274.obj has changed, recomputing SDF\n",
      "INFO: Computing SDF for 56f2cfa7d89ef32a5eef6c5d029c7274.obj\n",
      "INFO: Processing mesh 1/1: 56f2cfa7d89ef32a5eef6c5d029c7274.obj\n",
      "INFO: Loaded 56f2cfa7d89ef32a5eef6c5d029c7274.obj from cache\n",
      "INFO: Processing mesh 1/1: 56f2cfa7d89ef32a5eef6c5d029c7274.obj\n",
      "INFO: Loaded 56f2cfa7d89ef32a5eef6c5d029c7274.obj from cache\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import yaml\n",
    "sys.path.append(\"..\")  \n",
    "from sefmp.data.grasp_dataset import GraspDataset\n",
    "from sefmp.core.utils import load_config, get_device\n",
    "from sefmp.data.grasp_dataset import DataLoader, DataSelector, GraspDataModule\n",
    "from datetime import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "config_path = '../sefmp/configs/sanity_check.yaml'\n",
    "with open(config_path) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "device = get_device()\n",
    "\n",
    "# Setup unique run name if not specified\n",
    "if cfg[\"logging\"][\"run_name\"] is None:\n",
    "    cfg[\"logging\"][\"run_name\"] = (\n",
    "        f\"sefmp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "\n",
    "\n",
    "selector = DataSelector(\n",
    "            grasp_id=cfg[\"data\"].get(\"grasp_id\"),\n",
    "            object_id=cfg[\"data\"].get(\"object_id\"),\n",
    "            item_name=cfg[\"data\"].get(\"item_name\"),\n",
    "        )\n",
    "grasp_data = GraspDataModule(\n",
    "            data_root='../data',\n",
    "            selectors=selector,\n",
    "            sampler_opt='repeat',  # Using list of selectors\n",
    "            batch_size=16,\n",
    "            num_samples=2,  # Optional: limit total samples\n",
    "        )\n",
    "grasp_data.setup()\n",
    "\n",
    "# # Get data loaders\n",
    "# train_loader = grasp_data.train_dataloader()\n",
    "# val_loader = grasp_data.val_dataloader()\n",
    "\n",
    "# # Create a DataLoader instance\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# # Iterate through the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     # Process your batch\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8489dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from typeguard import typechecked\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from models.inr import INR\n",
    "\n",
    "\n",
    "def plot_image(\n",
    "    mlp_model: INR, device: torch.device\n",
    ") -> plt.Figure:  # Updated return type hint\n",
    "    resolution = 28\n",
    "    x = np.linspace(-1, 1, resolution)\n",
    "    y = np.linspace(-1, 1, resolution)\n",
    "    grid_x, grid_y = np.meshgrid(x, y)\n",
    "\n",
    "    inputs = np.stack([grid_x.ravel(), grid_y.ravel()], axis=-1)\n",
    "    inputs_tensor = torch.tensor(inputs, dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = mlp_model(inputs_tensor).cpu().numpy()\n",
    "\n",
    "    image = outputs.reshape(resolution, resolution)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image, cmap=\"gray\", extent=(-1, 1, -1, 1))\n",
    "    plt.axis(\"off\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def load_weights_into_inr(weights: Tensor, inr_model: INR) -> INR:\n",
    "    \"\"\"Helper function to load weights into INR model.\"\"\"\n",
    "    state_dict = {}\n",
    "    start_idx = 0\n",
    "    for key, param in inr_model.state_dict().items():\n",
    "        param_size = param.numel()\n",
    "        param_data = weights[start_idx : start_idx + param_size].reshape(param.shape)\n",
    "        state_dict[key] = param_data\n",
    "        start_idx += param_size\n",
    "    inr_model.load_state_dict(state_dict)\n",
    "    return inr_model\n",
    "\n",
    "\n",
    "def create_reconstruction_visualizations(\n",
    "    originals: Tensor,\n",
    "    reconstructions: Tensor,\n",
    "    inr_model: INR,\n",
    "    prefix: str,\n",
    "    batch_idx: int,\n",
    "    global_step: int,\n",
    "    is_fixed: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"Create visualization grid for original-reconstruction pairs.\"\"\"\n",
    "    result_dict = {}\n",
    "\n",
    "    # Create visualizations for each pair\n",
    "    for i, (orig, recon) in enumerate(zip(originals, reconstructions)):\n",
    "        # Generate figures\n",
    "        original_fig = plot_image(load_weights_into_inr(orig, inr_model), orig.device)\n",
    "        recon_fig = plot_image(load_weights_into_inr(recon, inr_model), recon.device)\n",
    "\n",
    "        # Add to result dictionary with unique keys\n",
    "        sample_type = \"fixed\" if is_fixed else \"batch\"\n",
    "        result_dict[f\"{prefix}/{sample_type}/original_{i}\"] = wandb.Image(original_fig)\n",
    "        result_dict[f\"{prefix}/{sample_type}/reconstruction_{i}\"] = wandb.Image(\n",
    "            recon_fig\n",
    "        )\n",
    "\n",
    "        # Close figures\n",
    "        plt.close(original_fig)\n",
    "        plt.close(recon_fig)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    @typechecked\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, z_dim: int, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    @typechecked\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch input_dim\"]\n",
    "    ) -> Float[Tensor, \"batch z_dim\"]:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    @typechecked\n",
    "    def __init__(self, z_dim: int, hidden_dim: int, output_dim: int, **kwargs):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    @typechecked\n",
    "    def forward(\n",
    "        self, z: Float[Tensor, \"batch z_dim\"]\n",
    "    ) -> Float[Tensor, \"batch output_dim\"]:\n",
    "        z = F.relu(self.fc1(z))\n",
    "        x_reconstructed = self.fc2(z)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    @typechecked\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(config)\n",
    "        self.config = config\n",
    "\n",
    "        # Initialize encoder and decoder\n",
    "        self.encoder = Encoder(**config[\"model\"])\n",
    "        self.decoder = Decoder(**config[\"model\"])\n",
    "\n",
    "        # Initialize fixed validation and training samples\n",
    "        self.fixed_val_samples: list[Tensor] | None = None\n",
    "        self.fixed_train_samples: list[Tensor] | None = None\n",
    "        self.fixed_sample_reconstructions: dict[str, list[Tensor]] = {}\n",
    "\n",
    "        # Store optimizer and scheduler config\n",
    "        self.optimizer_config = config[\"optimizer\"]\n",
    "        self.scheduler_config = config[\"scheduler\"]\n",
    "\n",
    "        # Initialize quality metrics\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "\n",
    "        # Create demo INR for visualization\n",
    "        self.demo_inr = INR(up_scale=16)\n",
    "        # Move demo INR to the same device as the model\n",
    "        self.demo_inr = self.demo_inr.to(self.device)\n",
    "\n",
    "    def setup(self, stage: str | None = None):\n",
    "        \"\"\"Setup fixed validation and training samples for tracking reconstruction progress.\"\"\"\n",
    "        if stage == \"fit\":\n",
    "            try:\n",
    "                num_samples = self.config[\"logging\"][\"num_samples_to_visualize\"]\n",
    "\n",
    "                # Setup validation samples\n",
    "                if (\n",
    "                    hasattr(self.trainer, \"val_dataloaders\")\n",
    "                    and self.trainer.val_dataloaders is not None\n",
    "                    and self.fixed_val_samples is None\n",
    "                ):\n",
    "                    val_batch = next(iter(self.trainer.val_dataloaders[0]))\n",
    "                    self.fixed_val_samples = val_batch[:num_samples].clone()\n",
    "\n",
    "                # Setup training samples\n",
    "                if (\n",
    "                    hasattr(self.trainer, \"train_dataloader\")\n",
    "                    and self.trainer.train_dataloader is not None\n",
    "                    and self.fixed_train_samples is None\n",
    "                ):\n",
    "                    train_batch = next(iter(self.trainer.train_dataloader()))\n",
    "                    self.fixed_train_samples = train_batch[:num_samples].clone()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not setup fixed samples: {e}\")\n",
    "                self.fixed_val_samples = None\n",
    "                self.fixed_train_samples = None\n",
    "\n",
    "    @typechecked\n",
    "    def encode(self, x: Float[Tensor, \"batch feature_dim\"]) -> Tensor:\n",
    "        return self.encoder(x)\n",
    "\n",
    "    @typechecked\n",
    "    def decode(\n",
    "        self, z: Float[Tensor, \"batch latent_dim\"]\n",
    "    ) -> Float[Tensor, \"batch feature_dim\"]:\n",
    "        return self.decoder(z)\n",
    "\n",
    "    @typechecked\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"batch feature_dim\"]\n",
    "    ) -> Float[Tensor, \"batch feature_dim\"]:\n",
    "        z = self.encode(input)\n",
    "        dec = self.decode(z)\n",
    "        return dec\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        inputs: Float[Tensor, \"batch feature_dim\"],\n",
    "        reconstructions: Float[Tensor, \"batch feature_dim\"],\n",
    "        prefix: str = \"train\",\n",
    "    ) -> Tuple[Tensor, dict[str, Tensor]]:\n",
    "        recon_loss = F.mse_loss(reconstructions, inputs)\n",
    "        return recon_loss, {f\"{prefix}/loss\": recon_loss}\n",
    "\n",
    "    def visualize_batch(self, batch: Tensor, prefix: str, batch_idx: int):\n",
    "        \"\"\"Visualize a batch of samples during training or validation.\"\"\"\n",
    "        if batch_idx % self.config[\"logging\"][\"log_every_n_steps\"] == 0:\n",
    "            with torch.no_grad():\n",
    "                reconstructions = self(batch)\n",
    "\n",
    "            # Log visualizations for a subset of the batch\n",
    "            num_samples = min(\n",
    "                self.config[\"logging\"][\"num_samples_to_visualize\"], batch.shape[0]\n",
    "            )  # Visualize up to num_samples_to_visualize\n",
    "            vis_dict = create_reconstruction_visualizations(\n",
    "                batch[:num_samples],\n",
    "                reconstructions[:num_samples],\n",
    "                self.demo_inr,\n",
    "                prefix,\n",
    "                batch_idx,\n",
    "                self.global_step,\n",
    "                is_fixed=False,\n",
    "            )\n",
    "\n",
    "            # Add step to wandb log\n",
    "            vis_dict[\"global_step\"] = self.global_step\n",
    "            self.logger.experiment.log(vis_dict)\n",
    "\n",
    "    def visualize_reconstructions(self, samples: Tensor, prefix: str, batch_idx: int):\n",
    "        \"\"\"Helper method to visualize fixed sample reconstructions during training or validation.\"\"\"\n",
    "        if (\n",
    "            samples is not None\n",
    "            and batch_idx % self.config[\"logging\"][\"log_every_n_steps\"] == 0\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                reconstructions = self(samples)\n",
    "\n",
    "            # Store reconstructions for this step\n",
    "            step_key = f\"{prefix}_step_{self.global_step}\"\n",
    "            self.fixed_sample_reconstructions[step_key] = reconstructions\n",
    "\n",
    "            # Create and log visualizations\n",
    "            vis_dict = create_reconstruction_visualizations(\n",
    "                samples,\n",
    "                reconstructions,\n",
    "                self.demo_inr,\n",
    "                prefix,\n",
    "                batch_idx,\n",
    "                self.global_step,\n",
    "                is_fixed=True,\n",
    "            )\n",
    "\n",
    "            # Add step to wandb log\n",
    "            vis_dict[\"global_step\"] = self.global_step\n",
    "            self.logger.experiment.log(vis_dict)\n",
    "\n",
    "    @typechecked\n",
    "    def training_step(\n",
    "        self, batch: Float[Tensor, \"batch feature_dim\"], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # Forward pass\n",
    "        reconstructions = self(batch)\n",
    "        loss, log_dict = self.compute_loss(batch, reconstructions, prefix=\"train\")\n",
    "\n",
    "        # Logging\n",
    "        self.log_dict(log_dict, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        # Log gradient norm\n",
    "        if batch_idx % self.config[\"trainer\"][\"log_every_n_steps\"] == 0:\n",
    "            total_norm = 0.0\n",
    "            for p in self.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm**0.5\n",
    "            self.log(\"train/grad_norm\", total_norm, prog_bar=False, sync_dist=True)\n",
    "\n",
    "        # Visualize both fixed samples and current batch\n",
    "        self.visualize_reconstructions(self.fixed_train_samples, \"train\", batch_idx)\n",
    "        self.visualize_batch(batch, \"train_batch\", batch_idx)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @typechecked\n",
    "    def validation_step(\n",
    "        self, batch: Float[Tensor, \"batch feature_dim\"], batch_idx: int\n",
    "    ) -> dict[str, Tensor]:\n",
    "        reconstructions = self(batch)\n",
    "        val_loss, val_log_dict = self.compute_loss(batch, reconstructions, prefix=\"val\")\n",
    "\n",
    "        # Log validation metrics\n",
    "        self.log_dict(val_log_dict, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        # Visualize both fixed samples and current batch\n",
    "        if (\n",
    "            batch_idx == 0\n",
    "            and self.current_epoch % self.config[\"logging\"][\"sample_every_n_epochs\"]\n",
    "            == 0\n",
    "        ):\n",
    "            self.visualize_reconstructions(self.fixed_val_samples, \"val\", batch_idx)\n",
    "            self.visualize_batch(batch, \"val_batch\", batch_idx)\n",
    "\n",
    "        return val_log_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Configure optimizer\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.optimizer_config[\"lr\"],\n",
    "            betas=tuple(self.optimizer_config[\"betas\"]),\n",
    "            eps=self.optimizer_config[\"eps\"],\n",
    "            weight_decay=self.optimizer_config[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        # Configure scheduler\n",
    "        if self.scheduler_config[\"name\"] == \"cosine\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=self.scheduler_config[\"T_max\"],\n",
    "                eta_min=self.scheduler_config[\"eta_min\"],\n",
    "            )\n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val/loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3cdaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3]), torch.Size([16, 3]), torch.Size([16, 32, 32, 32]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = grasp_data.train_dataloader()\n",
    "grasp_data.train_dataset.index\n",
    "for batch in train_loader:\n",
    "    print(len(batch))\n",
    "batch[0].shape, batch[1].shape, batch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from scipy.spatial.transform import Rotation\n",
    "from geomstats.geometry.special_orthogonal import SpecialOrthogonal\n",
    "\n",
    "\n",
    "# from utils.plotting import plot_so3\n",
    "# from utils.optimal_transport import so3_wasserstein as wasserstein\n",
    "# from FoldFlow.foldflow.utils.so3_helpers import norm_SO3, expmap\n",
    "# from FoldFlow.foldflow.utils.so3_condflowmatcher import SO3ConditionalFlowMatcher\n",
    "# from FoldFlow.so3_experiments.models.models import PMLP\n",
    "\n",
    "from sefmp.models.so3_helpers import norm_SO3, expmap\n",
    "from sefmp.models.so3_condflowmatcher import SO3ConditionalFlowMatcher\n",
    "from sefmp.models.pmlp import PMLP\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "# from data.datasets import SpecialOrthogonalGroup\n",
    "\n",
    "from geomstats._backend import _backend_config as _config\n",
    "_config.DEFAULT_DTYPE = torch.cuda.FloatTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83d3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "so3_group = SpecialOrthogonal(n=3, point_type=\"matrix\")\n",
    "FM = SO3ConditionalFlowMatcher(manifold=so3_group)\n",
    "def loss_fn(v, u, x):\n",
    "    res = v - u\n",
    "    norm = norm_SO3(x, res) # norm-squared on SO(3)\n",
    "    loss = torch.mean(norm, dim=-1)\n",
    "    return loss\n",
    "\n",
    "dim = 9 # network ouput is 9 dimensional (3x3 matrix)\n",
    "# MLP with a projection at the end, projection on to the tanget space of the manifold\n",
    "model = PMLP(dim=dim, time_varying=True) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "device = 'cpu'\n",
    "# ODE inference on SO(3)\n",
    "def inference(model, xt, t, dt):\n",
    "    with torch.no_grad():\n",
    "        vt = model(torch.cat([xt, t[:, None]], dim=-1)) # vt on the tanget of xt\n",
    "        vt = rearrange(vt, 'b (c d) -> b c d', c=3, d=3)\n",
    "        xt = rearrange(xt, 'b (c d) -> b c d', c=3, d=3)\n",
    "        xt_new = expmap(xt, vt * dt)                   # expmap to get the next point\n",
    "    return rearrange(xt_new, 'b c d -> b (c d)', c=3, d=3)\n",
    "# def inference_recursive(model, x_0, steps=100, device='cuda'):\n",
    "#     t = torch.linspace(0, 1, steps).to(device)\n",
    "#     def ode_func(t, xt,dt):\n",
    "#         # Reshape t to match model input expectations\n",
    "#         t_batch = torch.full((x.shape[0], 1), t.item(), device=device)\n",
    "#         with torch.no_grad():\n",
    "#             vt = model(torch.cat([xt, t[:, None]], dim=-1)) # vt on the tanget of xt\n",
    "#             vt = rearrange(vt, 'b (c d) -> b c d', c=3, d=3)\n",
    "#             xt = rearrange(xt, 'b (c d) -> b c d', c=3, d=3)\n",
    "#             xt_new = expmap(xt, vt * dt)                   # expmap to get the next point\n",
    "#         return rearrange(xt_new, 'b c d -> b (c d)', c=3, d=3)\n",
    "#         #return flow_model(x, t_batch)\n",
    "#     # Integrate from t=0 to t=1\n",
    "#     trajectory = odeint(\n",
    "#         ode_func,\n",
    "#         x_0,\n",
    "#         t,\n",
    "#         method='rk4'  # You can also try 'dopri5' for adaptive stepping\n",
    "#     )\n",
    "    \n",
    "#     return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811f038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example obj:  ../data/meshes/CerealBox/a61cd12446207107d59ff053d1480d84.obj\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meshes = glob.glob(\"../data/meshes/**/*.obj\")\n",
    "grasps = glob.glob(\"../data/grasps/*.h5\")\n",
    "example_obj= meshes[0]\n",
    "example_grasp = grasps[0]\n",
    "\n",
    "\n",
    "example_obj_id = example_obj.split(\"/\")[-1].split(\".\")[0]\n",
    "print(\"Example obj: \", example_obj)\n",
    "\n",
    "corresponding_grasps = [grasp for grasp in grasps if example_obj_id in grasp][0]\n",
    "\n",
    "with h5py.File(example_grasp, 'r') as h5file:\n",
    "    grasp_T = h5file['grasps']['transforms'][0,:,:]\n",
    "grasp_T = torch.tensor(grasp_T).unsqueeze(0).float()\n",
    "grasp_T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d6e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraspDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        so3_part = self.data[idx][:3,:3]\n",
    "        translational_part = self.data[idx][:3,3]\n",
    "        return so3_part, translational_part\n",
    "    \n",
    "grasp_dataset = GraspDataset(grasp_T.double())\n",
    "trainloader = DataLoader(grasp_dataset, batch_size=100, shuffle=True)\n",
    "testset = DataLoader(grasp_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30ae9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [02:11<00:00,  7.61it/s, Epoch=999, Loss=0.0863, Avg Loss=0.0863]\n"
     ]
    }
   ],
   "source": [
    "def main_loop(model, optimizer, num_epochs=1, display=True):\n",
    "    losses = []\n",
    "    global_step = 0\n",
    "    \n",
    "    # Create a single progress bar for all epochs\n",
    "    with tqdm(total=num_epochs * len(trainloader), desc=\"Training\") as global_progress_bar:\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            if (epoch % 10) == 0:\n",
    "                n_test = len(testset.dataset)\n",
    "                traj = torch.tensor(Rotation.random(n_test).as_matrix()).to(device).reshape(-1, 9)\n",
    "                for t in torch.linspace(0, 1, 200):\n",
    "                    t = torch.tensor([t]).to(device).repeat(n_test).requires_grad_(True)\n",
    "                    dt = torch.tensor([1/200]).to(device)\n",
    "                    traj = inference(model, traj, t, dt)\n",
    "                final_traj = rearrange(traj, 'b (c d) -> b c d', c=3, d=3)\n",
    "            \n",
    "            for _, (so3_data, trnslt_part) in enumerate(trainloader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Repeat the data if needed\n",
    "                so3_data = so3_data.repeat(1000, 1, 1)\n",
    "                x1 = so3_data.to(device).double()\n",
    "                x0 = torch.tensor(Rotation.random(x1.size(0)).as_matrix(), dtype=torch.float64).to(device)\n",
    "                \n",
    "                t, xt, ut = FM.sample_location_and_conditional_flow_simple(x0, x1)\n",
    "                \n",
    "                vt = model(torch.cat([rearrange(xt, 'b c d -> b (c d)', c=3, d=3), t[:, None]], dim=-1))\n",
    "                vt = rearrange(vt, 'b (c d) -> b c d', c=3, d=3)\n",
    "                \n",
    "                loss = loss_fn(vt, ut, xt)\n",
    "                epoch_losses.append(loss.detach().item())\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update the global progress bar\n",
    "                global_progress_bar.update(1)\n",
    "                global_progress_bar.set_postfix({\n",
    "                    'Epoch': epoch, \n",
    "                    'Loss': f'{loss.item():.4f}', \n",
    "                    'Avg Loss': f'{np.mean(epoch_losses):.4f}'\n",
    "                })\n",
    "                \n",
    "                global_step += 1\n",
    "    \n",
    "    return model, np.array(losses)\n",
    "\n",
    "# Run training\n",
    "model, losses = main_loop(model, optimizer, num_epochs=1000, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ffb578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2007,  0.3251, -0.9241],\n",
       "          [ 0.9171,  0.2694,  0.2940],\n",
       "          [ 0.3445, -0.9065, -0.2440]]]),\n",
       " tensor([[[-0.3808, -0.3274,  0.8647, -0.2228],\n",
       "          [-0.1488,  0.9447,  0.2922, -0.1666],\n",
       "          [-0.9126, -0.0174, -0.4085,  0.1002]]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = len(grasp_dataset)\n",
    "traj = torch.tensor(Rotation.random(n_test).as_matrix()).reshape(-1, 9)\n",
    "for t in torch.linspace(0, 1, 200):\n",
    "    t = torch.tensor([t],dtype=torch.float64).repeat(n_test)\n",
    "    dt = torch.tensor([1/200])\n",
    "    traj = inference(model, traj, t, dt)\n",
    "final_traj = rearrange(traj, 'b (c d) -> b c d', c=3, d=3)\n",
    "final_traj,grasp_dataset.data[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa0c4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE3VelocityField(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64): #trial for translation \n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, hidden_dim),  # Include time t as dim+1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)  # 3 for translation, we will implement SO3 later rt = expr0(tlogr0(r1)) with linalg inverse \n",
    "        )\n",
    "\n",
    "    def forward(self, T, t):\n",
    "        #T_flat = T.view(T.shape[0], -1)  # Flatten T\n",
    "        input_data = torch.cat([T,t ], dim=1)\n",
    "        return self.net(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17a2edc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 1.0780157394451935\n",
      "Epoch: 100 0.1735529375801334\n",
      "Epoch: 200 0.15793626708497058\n",
      "Epoch: 300 0.1419872366921193\n",
      "Epoch: 400 0.12431893279608217\n",
      "Epoch: 500 0.09352229379606569\n",
      "Epoch: 600 0.09848773438141888\n",
      "Epoch: 700 0.07494786140355303\n",
      "Epoch: 800 0.07107831549405334\n",
      "Epoch: 900 0.06115682387155495\n",
      "Epoch: 1000 0.06291244604575141\n",
      "Epoch: 1100 0.052585109247082984\n",
      "Epoch: 1200 0.05301342046876092\n",
      "Epoch: 1300 0.062307890074896344\n",
      "Epoch: 1400 0.050254247895819185\n",
      "Epoch: 1500 0.04238361949063993\n",
      "Epoch: 1600 0.056443570236101824\n",
      "Epoch: 1700 0.0417227896832179\n",
      "Epoch: 1800 0.049891370174069756\n",
      "Epoch: 1900 0.03843867549317297\n",
      "Epoch: 2000 0.04052325279315961\n",
      "Epoch: 2100 0.05866970867426109\n",
      "Epoch: 2200 0.03767757816371865\n",
      "Epoch: 2300 0.040966510002591425\n",
      "Epoch: 2400 0.03880663284367613\n",
      "Epoch: 2500 0.039523345338779166\n",
      "Epoch: 2600 0.037861939227961834\n",
      "Epoch: 2700 0.03878776995874324\n",
      "Epoch: 2800 0.03329922798936867\n",
      "Epoch: 2900 0.034159328225875\n",
      "Epoch: 3000 0.029399214354817498\n",
      "Epoch: 3100 0.028767872061355754\n",
      "Epoch: 3200 0.024390701249925133\n",
      "Epoch: 3300 0.032906470506222085\n",
      "Epoch: 3400 0.030926668876741635\n",
      "Epoch: 3500 0.03803487340722237\n",
      "Epoch: 3600 0.033715294610942435\n",
      "Epoch: 3700 0.025312884633242844\n",
      "Epoch: 3800 0.025704337171067914\n",
      "Epoch: 3900 0.03377632941434975\n",
      "Epoch: 4000 0.034350552690945446\n",
      "Epoch: 4100 0.026947225829837093\n",
      "Epoch: 4200 0.02610630801310068\n",
      "Epoch: 4300 0.027910758616054382\n",
      "Epoch: 4400 0.026382622776687712\n",
      "Epoch: 4500 0.026406452662314752\n",
      "Epoch: 4600 0.02792904675541692\n",
      "Epoch: 4700 0.02745734278959833\n",
      "Epoch: 4800 0.026267329561494142\n",
      "Epoch: 4900 0.016667836700566943\n",
      "Epoch: 5000 0.01831325830147558\n",
      "Epoch: 5100 0.022310101297723886\n",
      "Epoch: 5200 0.02263726070790229\n",
      "Epoch: 5300 0.019462113058870016\n",
      "Epoch: 5400 0.030646015877432358\n",
      "Epoch: 5500 0.023452999616061048\n",
      "Epoch: 5600 0.02449199900931507\n",
      "Epoch: 5700 0.018892221445331845\n",
      "Epoch: 5800 0.021038858616703418\n",
      "Epoch: 5900 0.018636070631225014\n",
      "Epoch: 6000 0.025526234334131533\n",
      "Epoch: 6100 0.01685842799157361\n",
      "Epoch: 6200 0.015618025544518337\n",
      "Epoch: 6300 0.021851944145789476\n",
      "Epoch: 6400 0.020212375517415688\n",
      "Epoch: 6500 0.01470490321986514\n",
      "Epoch: 6600 0.025976167130712522\n",
      "Epoch: 6700 0.01671642536478354\n",
      "Epoch: 6800 0.02162643620810357\n",
      "Epoch: 6900 0.013688739721137613\n",
      "Epoch: 7000 0.016540318317963917\n",
      "Epoch: 7100 0.024988115747049185\n",
      "Epoch: 7200 0.015590782284411221\n",
      "Epoch: 7300 0.019934569995058833\n",
      "Epoch: 7400 0.016958043217809308\n",
      "Epoch: 7500 0.015245556476277736\n",
      "Epoch: 7600 0.013124250383730449\n",
      "Epoch: 7700 0.013619018740090456\n",
      "Epoch: 7800 0.019915499736545737\n",
      "Epoch: 7900 0.013477453866685333\n",
      "Epoch: 8000 0.02009355149302978\n",
      "Epoch: 8100 0.021833868452036338\n",
      "Epoch: 8200 0.018470069134164636\n",
      "Epoch: 8300 0.020903746045202388\n",
      "Epoch: 8400 0.02433737719063184\n",
      "Epoch: 8500 0.012336311852080154\n",
      "Epoch: 8600 0.019505670039085828\n",
      "Epoch: 8700 0.017045806965765502\n",
      "Epoch: 8800 0.018302686082733558\n",
      "Epoch: 8900 0.01716424493341041\n",
      "Epoch: 9000 0.02043987407042483\n",
      "Epoch: 9100 0.018760786388712702\n",
      "Epoch: 9200 0.011510110767796485\n",
      "Epoch: 9300 0.013871121241263832\n",
      "Epoch: 9400 0.01880557476524157\n",
      "Epoch: 9500 0.02177058853645361\n",
      "Epoch: 9600 0.0187835885877175\n",
      "Epoch: 9700 0.015209230587739047\n",
      "Epoch: 9800 0.013032813360107066\n",
      "Epoch: 9900 0.020029728935933647\n",
      "Epoch: 10000 0.023383440312104417\n",
      "Epoch: 10100 0.020573124487778472\n",
      "Epoch: 10200 0.01167052055332018\n",
      "Epoch: 10300 0.011892849801166736\n",
      "Epoch: 10400 0.01587018392271943\n",
      "Epoch: 10500 0.01235561030963929\n",
      "Epoch: 10600 0.011366164487755617\n",
      "Epoch: 10700 0.016710367728493946\n",
      "Epoch: 10800 0.01907767263387371\n",
      "Epoch: 10900 0.018768320758240777\n",
      "Epoch: 11000 0.016572476448352046\n",
      "Epoch: 11100 0.008440032578213727\n",
      "Epoch: 11200 0.013876551481488133\n",
      "Epoch: 11300 0.018545064424873015\n",
      "Epoch: 11400 0.01266113697562282\n",
      "Epoch: 11500 0.015464732563484447\n",
      "Epoch: 11600 0.012588444127165461\n",
      "Epoch: 11700 0.0130038536329904\n",
      "Epoch: 11800 0.013370241427032812\n",
      "Epoch: 11900 0.012721617039833768\n",
      "Epoch: 12000 0.01851074968283313\n",
      "Epoch: 12100 0.013296141979784279\n",
      "Epoch: 12200 0.01576215116683904\n",
      "Epoch: 12300 0.015578021671923044\n",
      "Epoch: 12400 0.016333906546639933\n",
      "Epoch: 12500 0.012607471098638395\n",
      "Epoch: 12600 0.008893373093793835\n",
      "Epoch: 12700 0.01727103409445659\n",
      "Epoch: 12800 0.02180668386866909\n",
      "Epoch: 12900 0.012391781055751774\n",
      "Epoch: 13000 0.016107673199704152\n",
      "Epoch: 13100 0.01080037278064401\n",
      "Epoch: 13200 0.021180686262384306\n",
      "Epoch: 13300 0.009961810204070066\n",
      "Epoch: 13400 0.01151705640610929\n",
      "Epoch: 13500 0.018021125586677376\n",
      "Epoch: 13600 0.01562123236700637\n",
      "Epoch: 13700 0.017177566504761488\n",
      "Epoch: 13800 0.015576297989390154\n",
      "Epoch: 13900 0.012272547720812805\n",
      "Epoch: 14000 0.014170584023546326\n",
      "Epoch: 14100 0.012223085073863016\n",
      "Epoch: 14200 0.013551305540171579\n",
      "Epoch: 14300 0.013262374798572554\n",
      "Epoch: 14400 0.01337975427432977\n",
      "Epoch: 14500 0.014225767334916128\n",
      "Epoch: 14600 0.017340044024213783\n",
      "Epoch: 14700 0.01277776719048606\n",
      "Epoch: 14800 0.010943574388404797\n",
      "Epoch: 14900 0.010337335984128955\n",
      "Epoch: 15000 0.0129022100220815\n",
      "Epoch: 15100 0.012818253509429778\n",
      "Epoch: 15200 0.011758767302915947\n",
      "Epoch: 15300 0.01633871169074148\n",
      "Epoch: 15400 0.012848798776717454\n",
      "Epoch: 15500 0.009521743824667023\n",
      "Epoch: 15600 0.01014206577741136\n",
      "Epoch: 15700 0.009190442441538486\n",
      "Epoch: 15800 0.011132863036970522\n",
      "Epoch: 15900 0.010683285499807611\n",
      "Epoch: 16000 0.014315470923455332\n",
      "Epoch: 16100 0.008493938587443352\n",
      "Epoch: 16200 0.013144916799369794\n",
      "Epoch: 16300 0.016156074484635154\n",
      "Epoch: 16400 0.009556170472179326\n",
      "Epoch: 16500 0.014456890916872292\n",
      "Epoch: 16600 0.01039779951242731\n",
      "Epoch: 16700 0.011331995770624801\n",
      "Epoch: 16800 0.01026596685664304\n",
      "Epoch: 16900 0.010317422033939013\n",
      "Epoch: 17000 0.007217705580035787\n",
      "Epoch: 17100 0.010054776106484902\n",
      "Epoch: 17200 0.01040414894687893\n",
      "Epoch: 17300 0.015194015807766765\n",
      "Epoch: 17400 0.013256637423018733\n",
      "Epoch: 17500 0.011063743655934556\n",
      "Epoch: 17600 0.009989591179614057\n",
      "Epoch: 17700 0.020273487927274188\n",
      "Epoch: 17800 0.016299413421904746\n",
      "Epoch: 17900 0.01088104357695998\n",
      "Epoch: 18000 0.011010582706307738\n",
      "Epoch: 18100 0.011705851720887997\n",
      "Epoch: 18200 0.018434075508580058\n",
      "Epoch: 18300 0.013872235577463381\n",
      "Epoch: 18400 0.017239133047300938\n",
      "Epoch: 18500 0.008997249737092424\n",
      "Epoch: 18600 0.008622949710895427\n",
      "Epoch: 18700 0.007806051595760954\n",
      "Epoch: 18800 0.012262387890199067\n",
      "Epoch: 18900 0.007841469542825677\n",
      "Epoch: 19000 0.009163280932527688\n",
      "Epoch: 19100 0.010773025321404627\n",
      "Epoch: 19200 0.01445731684698066\n",
      "Epoch: 19300 0.009379802182028203\n",
      "Epoch: 19400 0.006760189800715467\n",
      "Epoch: 19500 0.009313650117758953\n",
      "Epoch: 19600 0.012528176339917233\n",
      "Epoch: 19700 0.007780226989224959\n",
      "Epoch: 19800 0.012984070364329043\n",
      "Epoch: 19900 0.009742053406587368\n",
      "Epoch: 20000 0.013291541703074856\n",
      "Epoch: 20100 0.010428396923204896\n",
      "Epoch: 20200 0.012788007168583587\n",
      "Epoch: 20300 0.010520704159817297\n",
      "Epoch: 20400 0.01062627517330928\n",
      "Epoch: 20500 0.010216393414457311\n",
      "Epoch: 20600 0.010272923456950259\n",
      "Epoch: 20700 0.00942020187652864\n",
      "Epoch: 20800 0.005392578339507915\n",
      "Epoch: 20900 0.011924248465422248\n",
      "Epoch: 21000 0.008977176232971\n",
      "Epoch: 21100 0.008533466770812948\n",
      "Epoch: 21200 0.012291548092924392\n",
      "Epoch: 21300 0.007433946964106288\n",
      "Epoch: 21400 0.01093493898950989\n",
      "Epoch: 21500 0.014274815479239531\n",
      "Epoch: 21600 0.016817827471175963\n",
      "Epoch: 21700 0.009227608922411328\n",
      "Epoch: 21800 0.01183015913331055\n",
      "Epoch: 21900 0.013339732021726\n",
      "Epoch: 22000 0.007542191340421614\n",
      "Epoch: 22100 0.009184773778861028\n",
      "Epoch: 22200 0.012512202844258859\n",
      "Epoch: 22300 0.012071527903756367\n",
      "Epoch: 22400 0.009268098857744626\n",
      "Epoch: 22500 0.008376755864651406\n",
      "Epoch: 22600 0.010003130954455685\n",
      "Epoch: 22700 0.007062375028982231\n",
      "Epoch: 22800 0.009892401831345798\n",
      "Epoch: 22900 0.01668848603841269\n",
      "Epoch: 23000 0.011737206172843384\n",
      "Epoch: 23100 0.010980239593591444\n",
      "Epoch: 23200 0.011328061352316008\n",
      "Epoch: 23300 0.00896541805754045\n",
      "Epoch: 23400 0.011126412300750909\n",
      "Epoch: 23500 0.01072998402601244\n",
      "Epoch: 23600 0.009457859239839656\n",
      "Epoch: 23700 0.01326014732004422\n",
      "Epoch: 23800 0.0071019598548304735\n",
      "Epoch: 23900 0.006785831734269447\n",
      "Epoch: 24000 0.011734939354098325\n",
      "Epoch: 24100 0.008926400449870971\n",
      "Epoch: 24200 0.012130107936479078\n",
      "Epoch: 24300 0.010143874803231074\n",
      "Epoch: 24400 0.008374301117195188\n",
      "Epoch: 24500 0.012809714989210104\n",
      "Epoch: 24600 0.011031281160553914\n",
      "Epoch: 24700 0.00839164230088632\n",
      "Epoch: 24800 0.007999967031039338\n",
      "Epoch: 24900 0.009521698853086211\n",
      "Epoch: 25000 0.008220195330960777\n",
      "Epoch: 25100 0.011136006343768389\n",
      "Epoch: 25200 0.009295286895792556\n",
      "Epoch: 25300 0.011511810369472096\n",
      "Epoch: 25400 0.01134369224907423\n",
      "Epoch: 25500 0.010538170364011024\n",
      "Epoch: 25600 0.010283885818139016\n",
      "Epoch: 25700 0.014466632433839619\n",
      "Epoch: 25800 0.015987998539556035\n",
      "Epoch: 25900 0.009468527573090697\n",
      "Epoch: 26000 0.007567659295355613\n",
      "Epoch: 26100 0.012317728160546115\n",
      "Epoch: 26200 0.006876083387717545\n",
      "Epoch: 26300 0.008002307004701954\n",
      "Epoch: 26400 0.007159041690866767\n",
      "Epoch: 26500 0.00723110281180487\n",
      "Epoch: 26600 0.0059918069207381824\n",
      "Epoch: 26700 0.007974465381134948\n",
      "Epoch: 26800 0.008764266077033976\n",
      "Epoch: 26900 0.011882329941935539\n",
      "Epoch: 27000 0.006260601796965977\n",
      "Epoch: 27100 0.004079425805148915\n",
      "Epoch: 27200 0.00887372790429574\n",
      "Epoch: 27300 0.013024661348638422\n",
      "Epoch: 27400 0.012032859489949916\n",
      "Epoch: 27500 0.006183940561120989\n",
      "Epoch: 27600 0.007440132151637637\n",
      "Epoch: 27700 0.0071993437032525625\n",
      "Epoch: 27800 0.00666740568430747\n",
      "Epoch: 27900 0.016257614388712094\n",
      "Epoch: 28000 0.012566639220902976\n",
      "Epoch: 28100 0.008598918584067246\n",
      "Epoch: 28200 0.008501542362102663\n",
      "Epoch: 28300 0.00951244141251141\n",
      "Epoch: 28400 0.015579743564598288\n",
      "Epoch: 28500 0.01209185510077061\n",
      "Epoch: 28600 0.010716834514632454\n",
      "Epoch: 28700 0.00790876774978422\n",
      "Epoch: 28800 0.010528284679364872\n",
      "Epoch: 28900 0.011447864830081637\n",
      "Epoch: 29000 0.00983438570335726\n",
      "Epoch: 29100 0.009783033554746107\n",
      "Epoch: 29200 0.009583355994200017\n",
      "Epoch: 29300 0.014342657990297995\n",
      "Epoch: 29400 0.0076719707017954735\n",
      "Epoch: 29500 0.00837253666124313\n",
      "Epoch: 29600 0.007946501272981611\n",
      "Epoch: 29700 0.012961402889012362\n",
      "Epoch: 29800 0.011664160868808797\n",
      "Epoch: 29900 0.014238604730945193\n",
      "Epoch: 30000 0.010594185051125407\n",
      "Epoch: 30100 0.009098440063366735\n",
      "Epoch: 30200 0.01349761129428758\n",
      "Epoch: 30300 0.009376126378649006\n",
      "Epoch: 30400 0.009358680263687238\n",
      "Epoch: 30500 0.009352008824167687\n",
      "Epoch: 30600 0.00846303963315178\n",
      "Epoch: 30700 0.014771370693411709\n",
      "Epoch: 30800 0.012306703206059694\n",
      "Epoch: 30900 0.008239443244347945\n",
      "Epoch: 31000 0.006409276238450981\n",
      "Epoch: 31100 0.011845110492026533\n",
      "Epoch: 31200 0.007532458481024214\n",
      "Epoch: 31300 0.008742287367705492\n",
      "Epoch: 31400 0.01613338283103625\n",
      "Epoch: 31500 0.009559146744895029\n",
      "Epoch: 31600 0.01131341828293757\n",
      "Epoch: 31700 0.005830863886435445\n",
      "Epoch: 31800 0.007046610481727914\n",
      "Epoch: 31900 0.0051465421785944635\n",
      "Epoch: 32000 0.008038097084196935\n",
      "Epoch: 32100 0.0049201977902042175\n",
      "Epoch: 32200 0.007531171931854699\n",
      "Epoch: 32300 0.008912065459306152\n",
      "Epoch: 32400 0.008270347541634271\n",
      "Epoch: 32500 0.010773020765889221\n",
      "Epoch: 32600 0.007529594627435058\n",
      "Epoch: 32700 0.005511757229196805\n",
      "Epoch: 32800 0.013062773828237357\n",
      "Epoch: 32900 0.005274671109710228\n",
      "Epoch: 33000 0.008083882485107335\n",
      "Epoch: 33100 0.011062594637414365\n",
      "Epoch: 33200 0.008915314068226238\n",
      "Epoch: 33300 0.011351008414040128\n",
      "Epoch: 33400 0.013043213938442936\n",
      "Epoch: 33500 0.008926784274755717\n",
      "Epoch: 33600 0.00788802375154301\n",
      "Epoch: 33700 0.009173972276449635\n",
      "Epoch: 33800 0.006924210449420929\n",
      "Epoch: 33900 0.009318738837680718\n",
      "Epoch: 34000 0.005784239951661509\n",
      "Epoch: 34100 0.00937973041620632\n",
      "Epoch: 34200 0.007808166946876826\n",
      "Epoch: 34300 0.0052611283567623335\n",
      "Epoch: 34400 0.008216387943826649\n",
      "Epoch: 34500 0.007441677531089378\n",
      "Epoch: 34600 0.009913770791064902\n",
      "Epoch: 34700 0.011094888375106332\n",
      "Epoch: 34800 0.00942297546288323\n",
      "Epoch: 34900 0.011079184253939618\n",
      "Epoch: 35000 0.013691712342875036\n",
      "Epoch: 35100 0.010506864629418545\n",
      "Epoch: 35200 0.008616469163673288\n",
      "Epoch: 35300 0.012263612193204803\n",
      "Epoch: 35400 0.009267679946481268\n",
      "Epoch: 35500 0.008502579291793502\n",
      "Epoch: 35600 0.01194325721211231\n",
      "Epoch: 35700 0.00673286521544996\n",
      "Epoch: 35800 0.01111520104636439\n",
      "Epoch: 35900 0.005590388386372661\n",
      "Epoch: 36000 0.010450972783236994\n",
      "Epoch: 36100 0.008371522944127407\n",
      "Epoch: 36200 0.007285785130582963\n",
      "Epoch: 36300 0.005637430695138993\n",
      "Epoch: 36400 0.0059487276463395964\n",
      "Epoch: 36500 0.005734963709629477\n",
      "Epoch: 36600 0.007918502065975769\n",
      "Epoch: 36700 0.008425176179388046\n",
      "Epoch: 36800 0.009315324667605533\n",
      "Epoch: 36900 0.011379425965827169\n",
      "Epoch: 37000 0.005821736573388317\n",
      "Epoch: 37100 0.004771314756557215\n",
      "Epoch: 37200 0.006979879827122063\n",
      "Epoch: 37300 0.005500779013888416\n",
      "Epoch: 37400 0.01020194290719034\n",
      "Epoch: 37500 0.007058639347082793\n",
      "Epoch: 37600 0.008017298567299776\n",
      "Epoch: 37700 0.005127223372608299\n",
      "Epoch: 37800 0.006743163750230148\n",
      "Epoch: 37900 0.005697594029883215\n",
      "Epoch: 38000 0.005749779828898564\n",
      "Epoch: 38100 0.009124740649803764\n",
      "Epoch: 38200 0.007648747951403284\n",
      "Epoch: 38300 0.008356747789464292\n",
      "Epoch: 38400 0.006565593699713202\n",
      "Epoch: 38500 0.006696141820886837\n",
      "Epoch: 38600 0.008099519682737701\n",
      "Epoch: 38700 0.008181345109400487\n",
      "Epoch: 38800 0.012890237644827815\n",
      "Epoch: 38900 0.008402476041722624\n",
      "Epoch: 39000 0.011011564964875764\n",
      "Epoch: 39100 0.009478051147585436\n",
      "Epoch: 39200 0.004551421091209916\n",
      "Epoch: 39300 0.010382171892772487\n",
      "Epoch: 39400 0.00860234301070775\n",
      "Epoch: 39500 0.010765600789448937\n",
      "Epoch: 39600 0.009831238633136964\n",
      "Epoch: 39700 0.010333799685274973\n",
      "Epoch: 39800 0.011526121739100505\n",
      "Epoch: 39900 0.008129262600088902\n",
      "Epoch: 40000 0.004929176020800524\n",
      "Epoch: 40100 0.004779772024932183\n",
      "Epoch: 40200 0.005924769313109336\n",
      "Epoch: 40300 0.018328393383980835\n",
      "Epoch: 40400 0.007207392774859597\n",
      "Epoch: 40500 0.010410006179895405\n",
      "Epoch: 40600 0.008014790633925835\n",
      "Epoch: 40700 0.00682631362844134\n",
      "Epoch: 40800 0.010003181797447256\n",
      "Epoch: 40900 0.01010246011650093\n",
      "Epoch: 41000 0.014344381360019141\n",
      "Epoch: 41100 0.005871947421369389\n",
      "Epoch: 41200 0.004395974513964453\n",
      "Epoch: 41300 0.012547039308632215\n",
      "Epoch: 41400 0.005410217616787654\n",
      "Epoch: 41500 0.006703423136167324\n",
      "Epoch: 41600 0.007895203828415234\n",
      "Epoch: 41700 0.008612358349313183\n",
      "Epoch: 41800 0.008079120359960536\n",
      "Epoch: 41900 0.004450022487733022\n",
      "Epoch: 42000 0.005555308436658622\n",
      "Epoch: 42100 0.008872027583330144\n",
      "Epoch: 42200 0.00841069486080916\n",
      "Epoch: 42300 0.007560024638002304\n",
      "Epoch: 42400 0.006545752406582796\n",
      "Epoch: 42500 0.009273961990546115\n",
      "Epoch: 42600 0.005112453456358281\n",
      "Epoch: 42700 0.008161021969457644\n",
      "Epoch: 42800 0.0078387285166863\n",
      "Epoch: 42900 0.005475273611781051\n",
      "Epoch: 43000 0.0068378025670340035\n",
      "Epoch: 43100 0.008664394971574953\n",
      "Epoch: 43200 0.007800121761878185\n",
      "Epoch: 43300 0.00865031143511443\n",
      "Epoch: 43400 0.010881257550051461\n",
      "Epoch: 43500 0.010441368222743918\n",
      "Epoch: 43600 0.005677786532111711\n",
      "Epoch: 43700 0.012303477781018686\n",
      "Epoch: 43800 0.009011094336720189\n",
      "Epoch: 43900 0.005974508973644606\n",
      "Epoch: 44000 0.009508046418586212\n",
      "Epoch: 44100 0.008355228226856203\n",
      "Epoch: 44200 0.006044944715518218\n",
      "Epoch: 44300 0.006606055764461983\n",
      "Epoch: 44400 0.008008060501219896\n",
      "Epoch: 44500 0.006942958104885217\n",
      "Epoch: 44600 0.010912494872565626\n",
      "Epoch: 44700 0.01136043148952877\n",
      "Epoch: 44800 0.007665693099555142\n",
      "Epoch: 44900 0.007296424630965913\n",
      "Epoch: 45000 0.005338996939970356\n",
      "Epoch: 45100 0.00944799933707363\n",
      "Epoch: 45200 0.006730881299528669\n",
      "Epoch: 45300 0.009605060109580123\n",
      "Epoch: 45400 0.007388626966370829\n",
      "Epoch: 45500 0.005887611587955203\n",
      "Epoch: 45600 0.005742459612165276\n",
      "Epoch: 45700 0.005938321877721322\n",
      "Epoch: 45800 0.013717054966900136\n",
      "Epoch: 45900 0.011433398261773926\n",
      "Epoch: 46000 0.00834937532024408\n",
      "Epoch: 46100 0.005284777039454829\n",
      "Epoch: 46200 0.008122998736874816\n",
      "Epoch: 46300 0.007115769974170526\n",
      "Epoch: 46400 0.006640364158746296\n",
      "Epoch: 46500 0.009184133146997283\n",
      "Epoch: 46600 0.009229961655058818\n",
      "Epoch: 46700 0.009242913732878664\n",
      "Epoch: 46800 0.005731525081772869\n",
      "Epoch: 46900 0.005215528973319515\n",
      "Epoch: 47000 0.012116557930199084\n",
      "Epoch: 47100 0.005683557825442667\n",
      "Epoch: 47200 0.006626153286883825\n",
      "Epoch: 47300 0.005716916786329054\n",
      "Epoch: 47400 0.007435843956032334\n",
      "Epoch: 47500 0.006943996276868126\n",
      "Epoch: 47600 0.005914464351122454\n",
      "Epoch: 47700 0.008102821521575895\n",
      "Epoch: 47800 0.007079055937348571\n",
      "Epoch: 47900 0.009762716529404069\n",
      "Epoch: 48000 0.007466736327214586\n",
      "Epoch: 48100 0.0065146599138715235\n",
      "Epoch: 48200 0.006252118176385965\n",
      "Epoch: 48300 0.015288094756059108\n",
      "Epoch: 48400 0.005571000454054955\n",
      "Epoch: 48500 0.0120604734045803\n",
      "Epoch: 48600 0.007805136858900107\n",
      "Epoch: 48700 0.003770680062466016\n",
      "Epoch: 48800 0.006454316831232928\n",
      "Epoch: 48900 0.006774506278101697\n",
      "Epoch: 49000 0.00697433972496384\n",
      "Epoch: 49100 0.00900906400104178\n",
      "Epoch: 49200 0.007407668247743738\n",
      "Epoch: 49300 0.007234319647358719\n",
      "Epoch: 49400 0.00730706517249795\n",
      "Epoch: 49500 0.010787064384536107\n",
      "Epoch: 49600 0.00477620780455765\n",
      "Epoch: 49700 0.005498115697770132\n",
      "Epoch: 49800 0.0062692473193290445\n",
      "Epoch: 49900 0.00977592555022137\n",
      "Epoch: 50000 0.008710896961658656\n",
      "Epoch: 50100 0.004996370199139423\n",
      "Epoch: 50200 0.004131620179126144\n",
      "Epoch: 50300 0.007159819701395129\n",
      "Epoch: 50400 0.012646174477862798\n",
      "Epoch: 50500 0.006743388805146028\n",
      "Epoch: 50600 0.0038561073688153933\n",
      "Epoch: 50700 0.008906419431278102\n",
      "Epoch: 50800 0.003500288653367755\n",
      "Epoch: 50900 0.005724277344092174\n",
      "Epoch: 51000 0.013747696042055555\n",
      "Epoch: 51100 0.009825415110175002\n",
      "Epoch: 51200 0.007234477975373462\n",
      "Epoch: 51300 0.0051685607182177995\n",
      "Epoch: 51400 0.006642216463148415\n",
      "Epoch: 51500 0.008243555510346275\n",
      "Epoch: 51600 0.00727354078540787\n",
      "Epoch: 51700 0.010636857311348957\n",
      "Epoch: 51800 0.009511399078170528\n",
      "Epoch: 51900 0.009045194365968965\n",
      "Epoch: 52000 0.011164979330617216\n",
      "Epoch: 52100 0.006362571043997356\n",
      "Epoch: 52200 0.007466376096160767\n",
      "Epoch: 52300 0.006491008619639315\n",
      "Epoch: 52400 0.0043644226986479795\n",
      "Epoch: 52500 0.004372381852802415\n",
      "Epoch: 52600 0.008433470497866295\n",
      "Epoch: 52700 0.01013201716993753\n",
      "Epoch: 52800 0.003150701571004709\n",
      "Epoch: 52900 0.007360118610652811\n",
      "Epoch: 53000 0.007342936438771001\n",
      "Epoch: 53100 0.007959742243177843\n",
      "Epoch: 53200 0.006909696679101429\n",
      "Epoch: 53300 0.008311968046169523\n",
      "Epoch: 53400 0.007192958988265966\n",
      "Epoch: 53500 0.012367543028319517\n",
      "Epoch: 53600 0.003925906865670474\n",
      "Epoch: 53700 0.0106498687492754\n",
      "Epoch: 53800 0.005577655186605352\n",
      "Epoch: 53900 0.006387113483306907\n",
      "Epoch: 54000 0.005709835576462617\n",
      "Epoch: 54100 0.008603721374805732\n",
      "Epoch: 54200 0.0059097361633508524\n",
      "Epoch: 54300 0.0057964031141698605\n",
      "Epoch: 54400 0.0075178460053800696\n",
      "Epoch: 54500 0.009416926127013425\n",
      "Epoch: 54600 0.007281030407004112\n",
      "Epoch: 54700 0.007995735291822553\n",
      "Epoch: 54800 0.012091503764041974\n",
      "Epoch: 54900 0.006979133632225808\n",
      "Epoch: 55000 0.003266695865325231\n",
      "Epoch: 55100 0.0038156163507310835\n",
      "Epoch: 55200 0.005813067217796731\n",
      "Epoch: 55300 0.005876938281734039\n",
      "Epoch: 55400 0.006409680894311262\n",
      "Epoch: 55500 0.010514602004656183\n",
      "Epoch: 55600 0.004634218483710334\n",
      "Epoch: 55700 0.008769320254207092\n",
      "Epoch: 55800 0.0055067739118962655\n",
      "Epoch: 55900 0.007095589038941015\n",
      "Epoch: 56000 0.005877816898446944\n",
      "Epoch: 56100 0.004650493462218746\n",
      "Epoch: 56200 0.006894204465006841\n",
      "Epoch: 56300 0.007944255501141705\n",
      "Epoch: 56400 0.007001116221925237\n",
      "Epoch: 56500 0.009356407411518752\n",
      "Epoch: 56600 0.009321394845693855\n",
      "Epoch: 56700 0.008500152109088683\n",
      "Epoch: 56800 0.006384750539074169\n",
      "Epoch: 56900 0.010926303651069644\n",
      "Epoch: 57000 0.004964710336727841\n",
      "Epoch: 57100 0.004750362710005743\n",
      "Epoch: 57200 0.005134869808981674\n",
      "Epoch: 57300 0.009487769433015632\n",
      "Epoch: 57400 0.009476244451563984\n",
      "Epoch: 57500 0.0055198495413526704\n",
      "Epoch: 57600 0.004435722692349493\n",
      "Epoch: 57700 0.007005748543580907\n",
      "Epoch: 57800 0.007973425766627486\n",
      "Epoch: 57900 0.0065883365189409356\n",
      "Epoch: 58000 0.01173349601377225\n",
      "Epoch: 58100 0.007746865823185609\n",
      "Epoch: 58200 0.011834802845662646\n",
      "Epoch: 58300 0.003365498399394178\n",
      "Epoch: 58400 0.007303956225129653\n",
      "Epoch: 58500 0.007409879624609655\n",
      "Epoch: 58600 0.005887907989093464\n",
      "Epoch: 58700 0.005916163745086227\n",
      "Epoch: 58800 0.00901121729775445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def conditional_flow_matching_loss(flow_model, x):\n",
    "    #Question: Should we calculate one for each time step or generate one time at a time?\n",
    "    \n",
    "    sigma_min = 1e-4\n",
    "    t = torch.rand(x.shape[0], device=x.device).unsqueeze(-1)\n",
    "    noise = torch.randn_like(x).to(x.device)\n",
    "\n",
    "    x_t = (1 - (1 - sigma_min) * t) * noise + t* x\n",
    "    optimal_flow = x - (1 - sigma_min) * noise\n",
    "    predicted_flow = flow_model(x_t, t)\n",
    "\n",
    "    return (predicted_flow - optimal_flow).square().mean()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SE3VelocityField().to(device)\n",
    "x = grasp_T[:, :3, 3]\n",
    "x_train = x.repeat(1000, 1).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100000):\n",
    "    model.zero_grad()\n",
    "    loss = conditional_flow_matching_loss(model,x_train)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}',loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8406c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2271, -0.1689,  0.1003]], grad_fn=<SelectBackward0>) tensor([[-0.2228, -0.1666,  0.1002]])\n"
     ]
    }
   ],
   "source": [
    "def run_flow(flow_model, x_0, steps=100, device='cuda'):\n",
    "    t = torch.linspace(0, 1, steps).to(device)\n",
    "    def ode_func(t, x):\n",
    "        # Reshape t to match model input expectations\n",
    "        t_batch = torch.full((x.shape[0], 1), t.item(), device=device)\n",
    "        return flow_model(x, t_batch)\n",
    "    # Integrate from t=0 to t=1\n",
    "    trajectory = odeint(\n",
    "        ode_func,\n",
    "        x_0,\n",
    "        t,\n",
    "        method='rk4'  # You can also try 'dopri5' for adaptive stepping\n",
    "    )\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "noise = torch.randn_like(grasp_T[:,:3,3]).to(device)\n",
    "trajectory = run_flow(model, noise, steps=100, device=device)\n",
    "print(trajectory[-1],grasp_T[:,:3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level groups: ['grasps', 'gripper', 'object']\n",
      "\n",
      "Exploring complete structure:\n",
      "\n",
      "=== grasps ===\n",
      "Group: qualities\n",
      "  Contents: ['flex']\n",
      "  Group: flex\n",
      "    Contents: ['object_in_gripper', 'object_motion_during_closing_angular', 'object_motion_during_closing_linear', 'object_motion_during_shaking_angular', 'object_motion_during_shaking_linear']\n",
      "    Dataset: object_in_gripper\n",
      "      Shape: (2000,)\n",
      "      Type: int64\n",
      "      First few values: [1 1]\n",
      "    Dataset: object_motion_during_closing_angular\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [0.54927611 0.15047622]\n",
      "    Dataset: object_motion_during_closing_linear\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [0.09023842 0.02939418]\n",
      "    Dataset: object_motion_during_shaking_angular\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [0.0185062  0.04676599]\n",
      "    Dataset: object_motion_during_shaking_linear\n",
      "      Shape: (2000,)\n",
      "      Type: float64\n",
      "      First few values: [0.00300531 0.00718987]\n",
      "Dataset: transforms\n",
      "  Shape: (2000, 4, 4)\n",
      "  Type: float64\n",
      "  First few values: [[[-0.38083587 -0.32743824  0.86472437 -0.22282051]\n",
      "  [-0.14879235  0.94471263  0.29219666 -0.16660974]\n",
      "  [-0.91259239 -0.01738541 -0.40850076  0.10019681]\n",
      "  [ 0.          0.          0.          1.        ]]\n",
      "\n",
      " [[-0.38083587  0.14879235  0.91259239 -0.22760731]\n",
      "  [-0.14879235  0.96424346 -0.21930658 -0.11545942]\n",
      "  [-0.91259239 -0.21930658 -0.34507934  0.09385467]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "\n",
      "=== gripper ===\n",
      "Dataset: configuration\n",
      "  Shape: (1,)\n",
      "  Type: float64\n",
      "  First few values: [0.04]\n",
      "Dataset: type\n",
      "  Shape: ()\n",
      "  Type: object\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "\n",
      "=== object ===\n",
      "Dataset: com\n",
      "  Shape: (3,)\n",
      "  Type: float64\n",
      "  First few values: [-0.01902643 -0.15787416]\n",
      "Dataset: density\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: file\n",
      "  Shape: ()\n",
      "  Type: object\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: friction\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: inertia\n",
      "  Shape: (3, 3)\n",
      "  Type: float64\n",
      "  First few values: [[6.797040e-05 1.622000e-07 5.730000e-08]\n",
      " [1.622000e-07 4.103991e-04 1.040000e-08]]\n",
      "Dataset: mass\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: scale\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n",
      "Dataset: volume\n",
      "  Shape: ()\n",
      "  Type: float64\n",
      "  Could not print values: Illegal slicing argument for scalar dataspace\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def explore_group(group, indent=\"\"):\n",
    "    \"\"\"Recursively explore an HDF5 group and its contents\"\"\"\n",
    "    for name, item in group.items():\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print(f\"{indent}Group: {name}\")\n",
    "            print(f\"{indent}  Contents: {list(item.keys())}\")\n",
    "            explore_group(item, indent + \"  \")\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(f\"{indent}Dataset: {name}\")\n",
    "            print(f\"{indent}  Shape: {item.shape}\")\n",
    "            print(f\"{indent}  Type: {item.dtype}\")\n",
    "            try:\n",
    "                print(f\"{indent}  First few values: {item[:2]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{indent}  Could not print values: {e}\")\n",
    "\n",
    "with h5py.File(example_grasp, 'r') as h5file:\n",
    "    print(\"Top-level groups:\", list(h5file.keys()))\n",
    "    \n",
    "    print(\"\\nExploring complete structure:\")\n",
    "    for top_group_name in h5file.keys():\n",
    "        print(f\"\\n=== {top_group_name} ===\")\n",
    "        top_group = h5file[top_group_name]\n",
    "        explore_group(top_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
